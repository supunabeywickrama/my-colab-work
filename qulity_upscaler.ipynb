{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNy/qkylV7vBYuWaoGU/ECL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supunabeywickrama/my-colab-work/blob/main/qulity_upscaler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohTPT2UEVI8q",
        "outputId": "16571897-dc94-4fc9-f06a-8e625627f705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m773.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# (Cell type: code)\n",
        "# Mount Drive (optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # accept prompt\n",
        "\n",
        "# Basic installs\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq ffmpeg\n",
        "\n",
        "# Python dependencies\n",
        "# We'll use PyTorch (GPU), torchvision, imageio, pillow, tqdm\n",
        "import sys\n",
        "!pip install --quiet torch torchvision==0.24.1 --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --quiet einops imageio tqdm opencv-python pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "import os\n",
        "WORKDIR = \"/content/vsr_project\"\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "\n",
        "# Paths\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/vsr_project\"   # change if needed\n",
        "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
        "\n",
        "DATA_DIR = os.path.join(WORKDIR, \"data\")\n",
        "HR_FRAMES = os.path.join(DATA_DIR, \"hr_frames\")\n",
        "LR_FRAMES = os.path.join(DATA_DIR, \"lr_frames\")\n",
        "MODEL_DIR = os.path.join(WORKDIR, \"models\")\n",
        "OUTPUT_DIR = os.path.join(WORKDIR, \"outputs\")\n",
        "\n",
        "for d in [DATA_DIR, HR_FRAMES, LR_FRAMES, MODEL_DIR, OUTPUT_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Input video path (you'll upload this later or set to a Drive path)\n",
        "INPUT_VIDEO = \"/content/input.mp4\"   # default, replace if using Drive file\n",
        "SCALE = 4                            # upscale factor for the pipeline (x2 or x4)\n",
        "BATCH_SIZE = 4                       # training batch size (temporal)\n",
        "SEQ_LEN = 5                          # number of frames in a training sample (odd recommended)\n",
        "DEVICE = \"cuda\" if (os.environ.get(\"COLAB_GPU\") is not None or \\\n",
        "                    __import__(\"torch\").cuda.is_available()) else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXcJvq1ZWaVK",
        "outputId": "dc7bcffb-ffce-4da7-d824-c48ba352d631"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "# Use Colab upload widget to upload a short HR video (e.g., 3-10 seconds)\n",
        "from google.colab import files\n",
        "print(\"Upload a short HR video (mp4). It will be saved at /content/input.mp4\")\n",
        "uploaded = files.upload()\n",
        "# If user uploaded file, set INPUT_VIDEO automatically\n",
        "if uploaded:\n",
        "    name = next(iter(uploaded.keys()))\n",
        "    print(\"Uploaded:\", name)\n",
        "    # Move to INPUT_VIDEO\n",
        "    import shutil\n",
        "    shutil.move(name, INPUT_VIDEO)\n",
        "print(\"INPUT_VIDEO path set to:\", INPUT_VIDEO)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "NcgFwtHuW-W6",
        "outputId": "75b1a42c-7524-4487-a1be-09d56fe9791c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload a short HR video (mp4). It will be saved at /content/input.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f68d520d-fed3-4bb9-b87f-576d33b96d77\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f68d520d-fed3-4bb9-b87f-576d33b96d77\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving input.mp4 to input.mp4\n",
            "Uploaded: input.mp4\n",
            "INPUT_VIDEO path set to: /content/input.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "import os, subprocess, shlex, glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Clean existing frames\n",
        "!rm -rf \"{HR_FRAMES}/*\" || true\n",
        "!rm -rf \"{LR_FRAMES}/*\" || true\n",
        "\n",
        "# Extract frames (PNG) and audio\n",
        "def extract_frames_and_audio(video_path, frames_dir, audio_out):\n",
        "    os.makedirs(frames_dir, exist_ok=True)\n",
        "    cmd = f'ffmpeg -y -i \"{video_path}\" \"{frames_dir}/frame_%06d.png\"'\n",
        "    print(\"Running:\", cmd)\n",
        "    subprocess.check_call(shlex.split(cmd))\n",
        "    # extract audio\n",
        "    cmd2 = f'ffmpeg -y -i \"{video_path}\" -vn -acodec copy \"{audio_out}\"'\n",
        "    try:\n",
        "        subprocess.check_call(shlex.split(cmd2))\n",
        "    except Exception as e:\n",
        "        print(\"Audio extraction failed (maybe no audio). Error:\", e)\n",
        "\n",
        "AUDIO_PATH = os.path.join(WORKDIR, \"audio.aac\")\n",
        "extract_frames_and_audio(INPUT_VIDEO, HR_FRAMES, AUDIO_PATH)\n",
        "\n",
        "num_hr = len(glob.glob(os.path.join(HR_FRAMES, \"frame_*.png\")))\n",
        "print(f\"Extracted {num_hr} HR frames to {HR_FRAMES}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdvIAj1cXR3e",
        "outputId": "da219be7-adcd-4dfb-d929-79a885da466c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: ffmpeg -y -i \"/content/input.mp4\" \"/content/vsr_project/data/hr_frames/frame_%06d.png\"\n",
            "Extracted 288 HR frames to /content/vsr_project/data/hr_frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "import random, numpy as np, cv2, os\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageFilter, ImageOps\n",
        "import imageio\n",
        "\n",
        "def degrade_image(hr_img, scale=SCALE, jpeg_q=None, add_noise=True, blur_sigma=None):\n",
        "    # hr_img: numpy uint8 HxWxC\n",
        "    img = hr_img.copy()\n",
        "    if blur_sigma is None:\n",
        "        blur_sigma = random.uniform(0.0, 1.6)  # mild motion / gaussian blur\n",
        "\n",
        "    if blur_sigma > 0:\n",
        "        img = cv2.GaussianBlur(img, (0,0), blur_sigma)\n",
        "\n",
        "    # downsample\n",
        "    h, w = img.shape[:2]\n",
        "    hr_h, hr_w = h, w\n",
        "    lr_h, lr_w = hr_h // scale, hr_w // scale\n",
        "    img_lr = cv2.resize(img, (lr_w, lr_h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Add sensor-like noise\n",
        "    if add_noise:\n",
        "        noise_level = random.uniform(0, 8)  # std dev\n",
        "        noise = np.random.normal(0, noise_level, img_lr.shape).astype(np.float32)\n",
        "        img_lr = np.clip(img_lr.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # JPEG compression\n",
        "    if jpeg_q is None:\n",
        "        jpeg_q = random.randint(30, 95)\n",
        "    # encode to jpeg and decode to simulate compression\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), jpeg_q]\n",
        "    _, encimg = cv2.imencode('.jpg', img_lr, encode_param)\n",
        "    img_lr = cv2.imdecode(encimg, 1)\n",
        "\n",
        "    return img_lr\n",
        "\n",
        "# Run degradation on all HR frames and save LR frames\n",
        "hr_paths = sorted(Path(HR_FRAMES).glob(\"frame_*.png\"))\n",
        "for i, p in enumerate(hr_paths):\n",
        "    hr = cv2.imread(str(p))[:,:,::-1]  # BGR->RGB\n",
        "    lr = degrade_image(hr, scale=SCALE)\n",
        "    outp = os.path.join(LR_FRAMES, f\"frame_{i:06d}.png\")\n",
        "    imageio.imsave(outp, lr)\n",
        "print(\"Saved LR frames to\", LR_FRAMES, \"count:\", len(list(Path(LR_FRAMES).glob(\"*.png\"))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWputpuDXWOn",
        "outputId": "a3ac0c27-25e5-4b67-e5b5-7bcb0c8e560e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved LR frames to /content/vsr_project/data/lr_frames count: 288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "        self.act = nn.ReLU(True)\n",
        "    def forward(self, x):\n",
        "        out = self.act(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        return x + 0.1 * out\n",
        "\n",
        "class SmallVSR(nn.Module):\n",
        "    def __init__(self, in_ch=3, feat=64, scale=4):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, feat, 3, 1, 1),\n",
        "            nn.ReLU(True),\n",
        "            ResidualBlock(feat),\n",
        "            ResidualBlock(feat)\n",
        "        )\n",
        "        # hidden state conv\n",
        "        self.state_conv = nn.Conv2d(feat*2, feat, 3, 1, 1)\n",
        "        # few residual blocks after fusion\n",
        "        self.res_blocks = nn.Sequential(*[ResidualBlock(feat) for _ in range(4)])\n",
        "        # upsampler\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Conv2d(feat, feat * (scale//2)**2, 3, 1, 1),\n",
        "            nn.PixelShuffle(scale//2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(feat, feat * 4, 3, 1, 1),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.Conv2d(feat, 3, 3, 1, 1)\n",
        "        )\n",
        "    def forward(self, frames):  # frames: tensor B, T, C, H, W  (LR)\n",
        "        B, T, C, H, W = frames.shape\n",
        "        device = frames.device\n",
        "        # initialize hidden state\n",
        "        h = torch.zeros(B, self.encoder[0].out_channels, H, W, device=device)\n",
        "        outputs = []\n",
        "        for t in range(T):\n",
        "            x = frames[:, t]  # B,C,H,W\n",
        "            feat = self.encoder(x)\n",
        "            # concat hidden & feat\n",
        "            fused = torch.cat([feat, h], dim=1)\n",
        "            h = F.relu(self.state_conv(fused))\n",
        "            h = self.res_blocks(h)\n",
        "            out = self.upsample(h)\n",
        "            outputs.append(out)\n",
        "        # return B, T, C, H*scale, W*scale\n",
        "        return torch.stack(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "_F4wz4Sbc6yU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, hr_dir, lr_dir, seq_len=SEQ_LEN, transform=None):\n",
        "        self.hr_paths = sorted(list(Path(hr_dir).glob(\"frame_*.png\")))\n",
        "        self.lr_paths = sorted(list(Path(lr_dir).glob(\"frame_*.png\")))\n",
        "        assert len(self.hr_paths) == len(self.lr_paths), \"HR/LR counts differ\"\n",
        "        self.N = len(self.hr_paths)\n",
        "        self.seq_len = seq_len\n",
        "        self.half = seq_len // 2\n",
        "        self.transform = transform or (lambda x: x)\n",
        "    def __len__(self):\n",
        "        return max(0, self.N - self.seq_len + 1)\n",
        "    def __getitem__(self, idx):\n",
        "        # sample sequence idx..idx+seq_len-1\n",
        "        hr_seq = []\n",
        "        lr_seq = []\n",
        "        for t in range(idx, idx + self.seq_len):\n",
        "            hr = Image.open(self.hr_paths[t]).convert(\"RGB\")\n",
        "            lr = Image.open(self.lr_paths[t]).convert(\"RGB\")\n",
        "            hr_seq.append(self.transform(hr))\n",
        "            lr_seq.append(self.transform(lr))\n",
        "        # stack to tensors shape (T,C,H,W)\n",
        "        hr = torch.stack(hr_seq, dim=0)\n",
        "        lr = torch.stack(lr_seq, dim=0)\n",
        "        # return lr (T,C,H,W), hr (T,C,H*scale,W*scale)\n",
        "        return lr, hr\n",
        "\n",
        "# transforms: to tensor and normalize [0,1]\n",
        "import torchvision.transforms.functional as Fv\n",
        "def pil_to_tensor(img):\n",
        "    arr = np.array(img).astype(np.float32) / 255.0\n",
        "    # HWC -> CHW\n",
        "    arr = np.transpose(arr, (2,0,1))\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "def transform_fn(pil_img):\n",
        "    return pil_to_tensor(pil_img)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = VideoFrameDataset(HR_FRAMES, LR_FRAMES, seq_len=SEQ_LEN, transform=transform_fn)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "print(\"Dataset length (sequences):\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhpniv5hdARl",
        "outputId": "0f1ed61a-0083-4431-a4a2-6091f04052e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length (sequences): 284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# Instantiate model\n",
        "model = SmallVSR(in_ch=3, feat=64, scale=SCALE).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# basic training loop (few epochs for demo)\n",
        "EPOCHS = 6\n",
        "save_every = 2\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
        "    for batch_idx, (lr_seq, hr_seq) in enumerate(pbar):\n",
        "        # lr_seq: B, T, C, H, W ; hr_seq: B, T, C, H*scale, W*scale\n",
        "        # move to device\n",
        "        lr_seq = lr_seq.to(DEVICE)\n",
        "        hr_seq = hr_seq.to(DEVICE)\n",
        "        # forward\n",
        "        out_seq = model(lr_seq)  # B, T, C, H*scale, W*scale\n",
        "        # compute loss only on central frame to reduce memory/training time (you can expand)\n",
        "        mid = out_seq.shape[1] // 2\n",
        "        loss = criterion(out_seq[:, mid], hr_seq[:, mid])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix(loss=running_loss / (batch_idx + 1))\n",
        "    # save checkpoint\n",
        "    if epoch % save_every == 0 or epoch == EPOCHS:\n",
        "        torch.save(model.state_dict(), os.path.join(MODEL_DIR, f\"smallvsr_epoch{epoch}.pth\"))\n",
        "        print(\"Saved checkpoint:\", os.path.join(MODEL_DIR, f\"smallvsr_epoch{epoch}.pth\"))\n",
        "print(\"Training finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJTy6_IVdIMp",
        "outputId": "812def6f-82d8-44b6-eb06-065886aac919"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/6: 100%|██████████| 71/71 [00:38<00:00,  1.86it/s, loss=0.103]\n",
            "Epoch 2/6: 100%|██████████| 71/71 [00:38<00:00,  1.84it/s, loss=0.0551]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint: /content/vsr_project/models/smallvsr_epoch2.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/6: 100%|██████████| 71/71 [00:40<00:00,  1.76it/s, loss=0.0476]\n",
            "Epoch 4/6: 100%|██████████| 71/71 [00:40<00:00,  1.77it/s, loss=0.0458]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint: /content/vsr_project/models/smallvsr_epoch4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/6: 100%|██████████| 71/71 [00:40<00:00,  1.76it/s, loss=0.0449]\n",
            "Epoch 6/6: 100%|██████████| 71/71 [00:40<00:00,  1.76it/s, loss=0.0443]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint: /content/vsr_project/models/smallvsr_epoch6.pth\n",
            "Training finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "import glob, imageio\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load latest checkpoint\n",
        "ckpts = sorted(glob.glob(os.path.join(MODEL_DIR, \"smallvsr_epoch*.pth\")))\n",
        "assert ckpts, \"No checkpoints found. Train first.\"\n",
        "ckpt = ckpts[-1]\n",
        "model.load_state_dict(torch.load(ckpt, map_location=DEVICE))\n",
        "model.to(DEVICE).eval()\n",
        "print(\"Loaded checkpoint:\", ckpt)\n",
        "\n",
        "# Read LR frames sorted\n",
        "lr_paths = sorted(Path(LR_FRAMES).glob(\"frame_*.png\"))\n",
        "N = len(lr_paths)\n",
        "\n",
        "# We'll run sliding windows to produce output frames for all central frames.\n",
        "pad = SEQ_LEN // 2\n",
        "# For boundary frames we repeat edge frames (simple padding)\n",
        "def load_img_tensor(path):\n",
        "    im = Image.open(path).convert(\"RGB\")\n",
        "    return pil_to_tensor(im)\n",
        "\n",
        "# create padded list of tensors\n",
        "tensors = [load_img_tensor(p) for p in lr_paths]\n",
        "# pad front/back\n",
        "for _ in range(pad):\n",
        "    tensors.insert(0, tensors[0])\n",
        "    tensors.append(tensors[-1])\n",
        "\n",
        "# run sliding windows\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"up_frames\"), exist_ok=True)\n",
        "out_paths = []\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(len(lr_paths))):\n",
        "        seq = tensors[i:i+SEQ_LEN]  # list of T tensors (C,H,W)\n",
        "        seq_t = torch.stack(seq, dim=0).unsqueeze(0).to(DEVICE)  # 1,T,C,H,W\n",
        "        out = model(seq_t)  # 1,T,C,H*s,W*s\n",
        "        mid = out.shape[1] // 2\n",
        "        frame = out[0, mid].cpu().clamp(0,1).numpy()  # C,H,W\n",
        "        frame = (frame * 255.0).transpose(1,2,0).astype('uint8')\n",
        "        out_path = os.path.join(OUTPUT_DIR, \"up_frames\", f\"frame_{i:06d}.png\")\n",
        "        imageio.imsave(out_path, frame)\n",
        "        out_paths.append(out_path)\n",
        "print(\"Upscaled frames saved to\", os.path.join(OUTPUT_DIR, \"up_frames\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEeXtPyWeHyL",
        "outputId": "ce85a19a-d4eb-48ef-e9d2-f22890a6a936"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint: /content/vsr_project/models/smallvsr_epoch6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 288/288 [00:51<00:00,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upscaled frames saved to /content/vsr_project/outputs/up_frames\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace previous final compose step with this robust version\n",
        "import subprocess, shlex, json, os\n",
        "\n",
        "INPUT_VIDEO = \"/content/input.mp4\"\n",
        "UPVIDEO = \"/content/vsr_project/outputs/upscaled_only.mp4\"\n",
        "OUT_SIDE_BY_SIDE = \"/content/vsr_project/outputs/side_by_side_output.mp4\"\n",
        "\n",
        "def ffprobe_stream_info(path):\n",
        "    # returns dict with width,height and whether audio stream exists\n",
        "    probe_w = None\n",
        "    probe_h = None\n",
        "    has_audio = False\n",
        "    try:\n",
        "        # video widthxheight\n",
        "        cmd = f'ffprobe -v error -select_streams v:0 -show_entries stream=width,height -of csv=p=0:s=x \"{path}\"'\n",
        "        out = subprocess.check_output(shlex.split(cmd)).decode().strip()\n",
        "        if out:\n",
        "            parts = out.split('x')\n",
        "            if len(parts) == 2:\n",
        "                probe_w = int(parts[0]); probe_h = int(parts[1])\n",
        "        # check audio streams\n",
        "        cmd2 = f'ffprobe -v error -select_streams a -show_entries stream=index -of csv=p=0 \"{path}\"'\n",
        "        out2 = subprocess.check_output(shlex.split(cmd2)).decode().strip()\n",
        "        has_audio = bool(out2)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"ffprobe error for\", path, \" — \", e)\n",
        "    return {\"width\": probe_w, \"height\": probe_h, \"has_audio\": has_audio}\n",
        "\n",
        "info_input = ffprobe_stream_info(INPUT_VIDEO)\n",
        "info_up = ffprobe_stream_info(UPVIDEO)\n",
        "\n",
        "print(\"Input video info:\", info_input)\n",
        "print(\"Upscaled video info:\", info_up)\n",
        "\n",
        "if info_up[\"height\"] is None:\n",
        "    raise RuntimeError(f\"Could not determine height of upscaled video: {UPVIDEO}\")\n",
        "\n",
        "# Build filter: scale input to match upscaled height exactly.\n",
        "# use -2 for width to keep aspect ratio while forcing even width (ffmpeg requirement for many codecs)\n",
        "scale_to = info_up[\"height\"]\n",
        "filter_complex = f\"[0:v]scale=-2:{scale_to}[left];[left][1:v]hstack=inputs=2[v]\"\n",
        "\n",
        "# Build ffmpeg command\n",
        "cmd = [\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-i\", INPUT_VIDEO,\n",
        "    \"-i\", UPVIDEO,\n",
        "    \"-filter_complex\", filter_complex,\n",
        "    \"-map\", \"[v]\"\n",
        "]\n",
        "\n",
        "# Only map audio if input has audio\n",
        "if info_input[\"has_audio\"]:\n",
        "    cmd += [\"-map\", \"0:a?\", \"-c:a\", \"copy\"]\n",
        "# Video codec options\n",
        "cmd += [\"-c:v\", \"libx264\", \"-crf\", \"18\", \"-preset\", \"medium\", OUT_SIDE_BY_SIDE]\n",
        "\n",
        "print(\"Running ffmpeg command:\")\n",
        "print(\" \".join(shlex.quote(x) for x in cmd))\n",
        "\n",
        "# Run and capture output\n",
        "proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "if proc.returncode != 0:\n",
        "    print(\"ffmpeg failed with return code\", proc.returncode)\n",
        "    print(\"----- ffmpeg stderr -----\")\n",
        "    print(proc.stderr)\n",
        "    print(\"----- ffmpeg stdout -----\")\n",
        "    print(proc.stdout)\n",
        "    raise RuntimeError(\"ffmpeg failed — see stderr above.\")\n",
        "else:\n",
        "    print(\"Success — side-by-side saved to:\", OUT_SIDE_BY_SIDE)\n",
        "    # show file size\n",
        "    if os.path.exists(OUT_SIDE_BY_SIDE):\n",
        "        sz_mb = os.path.getsize(OUT_SIDE_BY_SIDE) / (1024*1024)\n",
        "        print(f\"Output size: {sz_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no2TcUlEeXZL",
        "outputId": "5c071434-c6cb-413f-cb2f-cdc9ef4a1c8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input video info: {'width': 640, 'height': 320, 'has_audio': True}\n",
            "Upscaled video info: {'width': 640, 'height': 320, 'has_audio': False}\n",
            "Running ffmpeg command:\n",
            "ffmpeg -y -i /content/input.mp4 -i /content/vsr_project/outputs/upscaled_only.mp4 -filter_complex '[0:v]scale=-2:320[left];[left][1:v]hstack=inputs=2[v]' -map '[v]' -map '0:a?' -c:a copy -c:v libx264 -crf 18 -preset medium /content/vsr_project/outputs/side_by_side_output.mp4\n",
            "Success — side-by-side saved to: /content/vsr_project/outputs/side_by_side_output.mp4\n",
            "Output size: 2.57 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labeled side-by-side: \"Original\" (left) and \"Upscaled\" (right)\n",
        "import subprocess, shlex, json, os\n",
        "\n",
        "INPUT_VIDEO = \"/content/input.mp4\"\n",
        "UPVIDEO = \"/content/vsr_project/outputs/upscaled_only.mp4\"\n",
        "OUT_SIDE_BY_SIDE = \"/content/vsr_project/outputs/side_by_side_labeled.mp4\"\n",
        "\n",
        "# font path on Colab / Debian (DejaVu is available)\n",
        "FONT_PATH = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
        "if not os.path.exists(FONT_PATH):\n",
        "    FONT_PATH = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"  # fallback\n",
        "\n",
        "def ffprobe_stream_info(path):\n",
        "    probe_w = None\n",
        "    probe_h = None\n",
        "    has_audio = False\n",
        "    try:\n",
        "        cmd = f'ffprobe -v error -select_streams v:0 -show_entries stream=width,height -of csv=p=0:s=x \"{path}\"'\n",
        "        out = subprocess.check_output(shlex.split(cmd)).decode().strip()\n",
        "        if out:\n",
        "            parts = out.split('x')\n",
        "            if len(parts) == 2:\n",
        "                probe_w = int(parts[0]); probe_h = int(parts[1])\n",
        "        cmd2 = f'ffprobe -v error -select_streams a -show_entries stream=index -of csv=p=0 \"{path}\"'\n",
        "        out2 = subprocess.check_output(shlex.split(cmd2)).decode().strip()\n",
        "        has_audio = bool(out2)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"ffprobe error for\", path, \" — \", e)\n",
        "    return {\"width\": probe_w, \"height\": probe_h, \"has_audio\": has_audio}\n",
        "\n",
        "info_input = ffprobe_stream_info(INPUT_VIDEO)\n",
        "info_up = ffprobe_stream_info(UPVIDEO)\n",
        "\n",
        "print(\"Input video info:\", info_input)\n",
        "print(\"Upscaled video info:\", info_up)\n",
        "\n",
        "if info_up[\"height\"] is None:\n",
        "    raise RuntimeError(f\"Could not determine height of upscaled video: {UPVIDEO}\")\n",
        "\n",
        "scale_to = info_up[\"height\"]\n",
        "\n",
        "# Drawtext parameters (adjust fontsize as needed)\n",
        "label_fontsize = max(24, scale_to // 32)  # heuristic fontsize\n",
        "box_opacity = 0.6\n",
        "box_color = \"black@\" + str(box_opacity)\n",
        "fontfile_esc = FONT_PATH.replace(\":\", \"\\\\:\")  # escape if any colon\n",
        "\n",
        "# filter: scale input to match height, add label; add label to upscaled; then hstack\n",
        "# Use x=10 y=10 so text is in top-left with a semi-opaque box\n",
        "left_draw = (\n",
        "    f\"scale=-2:{scale_to},\"\n",
        "    f\"drawtext=fontfile='{fontfile_esc}':text='Original':x=10:y=10:fontsize={label_fontsize}:\"\n",
        "    f\"fontcolor=white:box=1:boxcolor={box_color}:boxborderw=12\"\n",
        ")\n",
        "right_draw = (\n",
        "    f\"drawtext=fontfile='{fontfile_esc}':text='Upscaled':x=10:y=10:fontsize={label_fontsize}:\"\n",
        "    f\"fontcolor=white:box=1:boxcolor={box_color}:boxborderw=12\"\n",
        ")\n",
        "\n",
        "filter_complex = f\"[0:v]{left_draw}[left];[1:v]{right_draw}[right];[left][right]hstack=inputs=2[v]\"\n",
        "\n",
        "cmd = [\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-i\", INPUT_VIDEO,\n",
        "    \"-i\", UPVIDEO,\n",
        "    \"-filter_complex\", filter_complex,\n",
        "    \"-map\", \"[v]\"\n",
        "]\n",
        "\n",
        "# Only map audio if input has audio\n",
        "if info_input[\"has_audio\"]:\n",
        "    cmd += [\"-map\", \"0:a?\", \"-c:a\", \"copy\"]\n",
        "\n",
        "# Video codec options\n",
        "cmd += [\"-c:v\", \"libx264\", \"-crf\", \"18\", \"-preset\", \"medium\", OUT_SIDE_BY_SIDE]\n",
        "\n",
        "print(\"Running ffmpeg command:\")\n",
        "print(\" \".join(shlex.quote(x) for x in cmd))\n",
        "\n",
        "proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "if proc.returncode != 0:\n",
        "    print(\"ffmpeg failed with return code\", proc.returncode)\n",
        "    print(\"----- ffmpeg stderr -----\")\n",
        "    print(proc.stderr)\n",
        "    print(\"----- ffmpeg stdout -----\")\n",
        "    print(proc.stdout)\n",
        "    raise RuntimeError(\"ffmpeg failed — see stderr above.\")\n",
        "else:\n",
        "    print(\"Success — labeled side-by-side saved to:\", OUT_SIDE_BY_SIDE)\n",
        "    if os.path.exists(OUT_SIDE_BY_SIDE):\n",
        "        sz_mb = os.path.getsize(OUT_SIDE_BY_SIDE) / (1024*1024)\n",
        "        print(f\"Output size: {sz_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ENju5wXkHfh",
        "outputId": "0e613603-e408-40f3-f6a5-9853aa1c135e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input video info: {'width': 640, 'height': 320, 'has_audio': True}\n",
            "Upscaled video info: {'width': 640, 'height': 320, 'has_audio': False}\n",
            "Running ffmpeg command:\n",
            "ffmpeg -y -i /content/input.mp4 -i /content/vsr_project/outputs/upscaled_only.mp4 -filter_complex '[0:v]scale=-2:320,drawtext=fontfile='\"'\"'/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'\"'\"':text='\"'\"'Original'\"'\"':x=10:y=10:fontsize=24:fontcolor=white:box=1:boxcolor=black@0.6:boxborderw=12[left];[1:v]drawtext=fontfile='\"'\"'/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'\"'\"':text='\"'\"'Upscaled'\"'\"':x=10:y=10:fontsize=24:fontcolor=white:box=1:boxcolor=black@0.6:boxborderw=12[right];[left][right]hstack=inputs=2[v]' -map '[v]' -map '0:a?' -c:a copy -c:v libx264 -crf 18 -preset medium /content/vsr_project/outputs/side_by_side_labeled.mp4\n",
            "Success — labeled side-by-side saved to: /content/vsr_project/outputs/side_by_side_labeled.mp4\n",
            "Output size: 2.58 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Cell type: code)\n",
        "# Clone Real-ESRGAN\n",
        "%cd /content\n",
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Create weights dir\n",
        "%cd /content/Real-ESRGAN\n",
        "!mkdir -p weights\n",
        "\n",
        "# Upload Real-ESRGAN weights using Colab file upload (if you have them locally),\n",
        "# or copy from Drive: e.g., !cp /content/drive/MyDrive/weights/RealESRGAN_x4plus.pth ./weights/\n",
        "from google.colab import files\n",
        "print(\"If you have a weights file locally, upload it now (RealESRGAN_x4plus.pth recommended).\")\n",
        "# files.upload()  # uncomment to use interactive upload\n",
        "\n",
        "# Run inference (example for x4plus)\n",
        "%cd /content/Real-ESRGAN\n",
        "# Replace MODEL_NAME with the model you have. This command takes frames_dir and outputs to specified dir.\n",
        "MODEL_NAME = \"RealESRGAN_x4plus\"\n",
        "INPUT_FRAMES_DIR = LR_FRAMES\n",
        "OUTPUT_FRAMES_DIR = os.path.join(OUTPUT_DIR, \"realesrgan_up\")\n",
        "!python inference_realesrgan.py -n {MODEL_NAME} -i \"{INPUT_FRAMES_DIR}\" -o \"{OUTPUT_FRAMES_DIR}\" --suffix \"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyHtJEMYgEeP",
        "outputId": "2a0df6f5-6892-4ce8-ff0b-a5b46fed1084"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'Real-ESRGAN' already exists and is not an empty directory.\n",
            "/content/Real-ESRGAN\n",
            "Requirement already satisfied: basicsr>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: facexlib>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: gfpgan>=1.3.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.12.0.88)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (11.3.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.9.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (0.24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (1.7.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (1.16.3)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (2.21.0a20251023)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.12/dist-packages (from basicsr>=1.4.2->-r requirements.txt (line 1)) (0.43.0)\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.12/dist-packages (from facexlib>=0.2.5->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from facexlib>=0.2.5->-r requirements.txt (line 2)) (0.60.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7->-r requirements.txt (line 7)) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->facexlib>=0.2.5->-r requirements.txt (line 2)) (0.43.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->basicsr>=1.4.2->-r requirements.txt (line 1)) (2025.11.12)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->basicsr>=1.4.2->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tb-nightly->basicsr>=1.4.2->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->basicsr>=1.4.2->-r requirements.txt (line 1)) (4.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy->facexlib>=0.2.5->-r requirements.txt (line 2)) (1.17.0)\n",
            "/content/Real-ESRGAN\n",
            "If you have a weights file locally, upload it now (RealESRGAN_x4plus.pth recommended).\n",
            "/content/Real-ESRGAN\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Real-ESRGAN/inference_realesrgan.py\", line 5, in <module>\n",
            "    from basicsr.archs.rrdbnet_arch import RRDBNet\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/__init__.py\", line 4, in <module>\n",
            "    from .data import *\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/data/__init__.py\", line 22, in <module>\n",
            "    _dataset_modules = [importlib.import_module(f'basicsr.data.{file_name}') for file_name in dataset_filenames]\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/data/realesrgan_dataset.py\", line 11, in <module>\n",
            "    from basicsr.data.degradations import circular_lowpass_kernel, random_mixed_kernels\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/basicsr/data/degradations.py\", line 8, in <module>\n",
            "    from torchvision.transforms.functional_tensor import rgb_to_grayscale\n",
            "ModuleNotFoundError: No module named 'torchvision.transforms.functional_tensor'\n"
          ]
        }
      ]
    }
  ]
}